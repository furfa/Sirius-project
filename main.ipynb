{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,ClassifierMixin, TransformerMixin, clone, RegressorMixin\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def VotingFunc(predictions1,classes,weights = {}):\n",
    "    predictions = np.column_stack(np.array(pr) for pr in predictions1)\n",
    "    ans = []\n",
    "    class_to_ind = {classes[i]:i for i in range(len(classes))}\n",
    "        \n",
    "    for pred1 in predictions:\n",
    "        pred = pred1\n",
    "        for p in range(pred.shape[0]):\n",
    "            pred[p] = class_to_ind[pred[p]]\n",
    "        counter = np.bincount(pred.astype(\"int64\"))\n",
    "        for we in weights.keys():\n",
    "            if class_to_ind[we]<counter.shape[0]:\n",
    "                counter[class_to_ind[we]]*=weights[we]\n",
    "        ans.append(classes[np.argmax(counter)])\n",
    "    return np.array(ans)\n",
    "\n",
    "class SimpleVoting(BaseEstimator,ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        ans = []\n",
    "        classes = self.models_[0].classes_\n",
    "        class_to_ind = {classes[i]:i for i in range(len(classes))}\n",
    "        for pred1 in predictions:\n",
    "            pred = pred1\n",
    "            for p in range(pred.shape[0]):\n",
    "                pred[p] = class_to_ind[pred[p]]\n",
    "            counter = np.bincount(pred)\n",
    "            ans.append(classes[np.argmax(counter)])\n",
    "        return np.array(ans) \n",
    "class SoftVoting(BaseEstimator,ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        predictions = self.models_[0].predict_proba(X) \n",
    "        for i,model in enumerate(self.models_):\n",
    "            if i>0:\n",
    "                predictions+=model.predict_proba(X) \n",
    "        predictions = np.argmax(predictions,axis = 1)\n",
    "        classes = self.models_[0].classes_\n",
    "        predictions = np.apply_along_axis(lambda x: classes[x], 0, predictions)\n",
    "        return predictions \n",
    "class WeightVoting(BaseEstimator,ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, models,weights):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        ans = []\n",
    "        classes = self.models_[0].classes_\n",
    "        class_to_ind = {classes[i]:i for i in range(len(classes))}\n",
    "        \n",
    "        for pred1 in predictions:\n",
    "            pred = pred1\n",
    "            for p in range(pred.shape[0]):\n",
    "                pred[p] = class_to_ind[pred[p]]\n",
    "            counter = np.bincount(pred)\n",
    "            for we in self.weights.keys():\n",
    "                if class_to_ind[we]<counter.shape[0]:\n",
    "                    counter[class_to_ind[we]]*=self.weights[we]\n",
    "            ans.append(classes[np.argmax(counter)])\n",
    "        return np.array(ans) \n",
    "class MeanModel(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1) \n",
    "\n",
    "class StackingWithMeanRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        self.trained_base_models = [list() for x in self.base_models]\n",
    "        #массив в каждой ячейке которого будет по n_folds базовых моделей обученных на n_folds-1 фолдах\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        # мета признаки\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, base_model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(base_model)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                self.trained_base_models[i].append(instance)\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Обучение мета_модели\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, X):\n",
    "        # мета признаки\n",
    "        meta_features = np.zeros((X.shape[0], len(self.trained_base_models)))\n",
    "        for j,model_type in enumerate(self.trained_base_models):\n",
    "            model_type_pred = np.zeros((X.shape[0], len(model_type)))\n",
    "            for i,model in enumerate(model_type):\n",
    "                model_type_pred[:,i] = model.predict(X)\n",
    "            meta_features[:,j] = model_type_pred.mean(axis=1)#Усреднение результатов моделей, обученных на разных фолдах\n",
    "        y_pred = self.meta_model_.predict(meta_features)\n",
    "        return y_pred\n",
    "class StackingWithVotingClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        self.trained_base_models = [list() for x in self.base_models]\n",
    "        #массив в каждой ячейке которого будет по n_folds базовых моделей обученных на n_folds-1 фолдах\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        # мета признаки\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, base_model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(base_model)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                self.trained_base_models[i].append(instance)\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Обучение мета_модели\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, X):\n",
    "        # мета признаки\n",
    "        meta_features = np.zeros((X.shape[0], len(self.trained_base_models)))\n",
    "        for j,model_type in enumerate(self.trained_base_models):\n",
    "            model_type_pred = np.zeros((X.shape[0], len(model_type)))\n",
    "            for i,model in enumerate(model_type):\n",
    "                model_type_pred[:,i] = model.predict(X)\n",
    "            \n",
    "            meta_features[:,j] = VotingFunc([model_type_pred[:,i] for i in range(len(self.trained_base_models))],classes=self.trained_base_models[0][0].classes_)\n",
    "            #Голосование результатов моделей, обученных на разных фолдах\n",
    "        y_pred = self.meta_model_.predict(meta_features)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.93 (+/- 0.05) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.05) [HardVoting]\n",
      "Accuracy: 0.95 (+/- 0.04) [SoftVoting]\n",
      "Accuracy: 0.95 (+/- 0.05) [WeightVoting]\n",
      "Accuracy: 0.95 (+/- 0.03) [Stacking]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1, max_depth=100000)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = XGBClassifier()\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "eclf = SimpleVoting([clf1,clf2,clf3])\n",
    "eclf1 = SoftVoting([clf1,clf2,clf3])\n",
    "eclf2 = WeightVoting([clf1,clf2,clf3],weights={2:1})\n",
    "eclf3 = StackingWithVotingClassifier(base_models=[clf1,clf2,clf3],meta_model=clf4)\n",
    "\n",
    "labels = ['Logistic Regression', 'Random Forest', 'Naive Bayes', 'HardVoting','SoftVoting',\"WeightVoting\",\"Stacking\"]\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf,eclf1,eclf2,eclf3], labels):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=5, \n",
    "                                              scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Accuracy: 0.90 (+/- 0.05) [Logistic Regression]\n",
      "Accuracy: 0.67 (+/- 0.18) [SGD]\n",
      "Accuracy: 0.91 (+/- 0.04) [Naive Bayes]\n",
      "Accuracy: 0.93 (+/- 0.03) [HardVoting]\n",
      "Accuracy: 0.90 (+/- 0.04) [SoftVoting]\n",
      "Accuracy: 0.93 (+/- 0.03) [WeightVoting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = SGDClassifier(loss = \"log\",random_state=2)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "eclf = SimpleVoting([clf1,clf2,clf3])\n",
    "eclf1 = SoftVoting([clf1,clf2,clf3])\n",
    "eclf2 = WeightVoting([clf1,clf2,clf3],weights={2:1})\n",
    "\n",
    "labels = ['Logistic Regression', 'SGD', 'Naive Bayes', 'HardVoting','SoftVoting',\"WeightVoting\"]\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf,eclf1,eclf2], labels):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=5, \n",
    "                                              scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
